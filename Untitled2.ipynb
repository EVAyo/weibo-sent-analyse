{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time usage: 0:00:00\n",
      "(4450, 300, 4449)\n",
      "Embedding(4450, 300, padding_idx=4449)\n",
      "<bound method Module.parameters of Model(\n",
      "  (embedding): Embedding(4450, 300, padding_idx=4449)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv2d(1, 256, kernel_size=(1, 300), stride=(1, 1))\n",
      "    (1): Conv2d(1, 256, kernel_size=(2, 300), stride=(1, 1))\n",
      "    (2): Conv2d(1, 256, kernel_size=(3, 300), stride=(1, 1))\n",
      "    (3): Conv2d(1, 256, kernel_size=(4, 300), stride=(1, 1))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (fc): Linear(in_features=1024, out_features=7, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "from distutils.command.config import config\n",
    "import time\n",
    "from certifi import contents\n",
    "from sklearn import datasets\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "#from importlib import import_module\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "import os\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "from tkinter import *\n",
    "from tkinter import messagebox\n",
    "MAX_VOCAB_SIZE = 10000  # 词表长度限制\n",
    "UNK, PAD = '<UNK>', '<PAD>'  # 未知字，padding符号\n",
    "\n",
    "class Config(object):\n",
    "    \"\"\"配置参数\"\"\"\n",
    "    def __init__(self, dataset, embedding):#random\n",
    "        \n",
    "        # self.model_name = 'TextCNN'\n",
    "        #self.train_path = '/datasets/5fbcdfa05005208e83d1ede4-momodel/news'                                # 训练集\n",
    "        self.train_path = '/datasets/weibo_senti_100k'             \n",
    "        # self.dev_path = dataset + '/data/dev.txt'                                    # 验证集\n",
    "        # self.test_path = dataset + '/data/test.txt'                                  # 测试集\n",
    "        # self.class_list = [x.strip() for x in open(\n",
    "        #     dataset + '/data/class.txt', encoding='utf-8').readlines()]              # 类别名单\n",
    "        self.vocab_path = 'mydata/data/vocab3.pkl'                                # 词表\n",
    "        self.batch_size = 128                                           # mini-batch大小\n",
    "        self.pad_size = 128\n",
    "        self.embedding_pretrained = None\n",
    "        self.n_vocab = 0\n",
    "        self.embed =300\n",
    "        self.filter_sizes = (1,2, 3, 4)                                   # 卷积核尺寸\n",
    "        self.num_filters = 256                                          # 卷积核数量(channels数)    \n",
    "        self.dropout = 0.4  \n",
    "        self.class_list = [x.strip() for x in open(\n",
    "            dataset + '/data/class3.txt', encoding='utf-8').readlines()]\n",
    "        self.num_classes = len(self.class_list)    \n",
    "        self.device = torch.device('cpu')   # 设备 \n",
    "        self.learning_rate = 1e-3                                       # 学习率\n",
    "        self.num_epochs = 30                                            # epoch数\n",
    "        self.save_path = dataset + '/saved_dict/' + 'cnn6' +''+ '.ckpt'\n",
    "        self.log_path = dataset + '/log/' + 'cnn'\n",
    "        self.require_improvement = 1000 \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(config.n_vocab, config.embed, padding_idx=config.n_vocab - 1)\n",
    "        print((config.n_vocab, config.embed, config.n_vocab - 1))\n",
    "        print(self.embedding)\n",
    "            #\n",
    "            #input()\n",
    "        self.convs = nn.ModuleList(#layer = nn.ModuleList([nn.Conv2d(in_channels=128, out_channels=64,kernel_size])\n",
    "            [nn.Conv2d(1, config.num_filters, (k, config.embed)) for k in config.filter_sizes])\n",
    "            \n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.fc = nn.Linear(config.num_filters * len(config.filter_sizes), config.num_classes)\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        z = conv(x)\n",
    "        # print(z)\n",
    "        # print(z.size())\n",
    "        # input()\n",
    "        x = F.relu(conv(x)).squeeze(3)\n",
    "        # print(\"eelu之后\")\n",
    "        # print(x)\n",
    "        # print(x.size())\n",
    "        # input()\n",
    "        #x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        x = F.avg_pool1d(x, x.size(2)).squeeze(2)\n",
    "        # print(\"   ---\")\n",
    "        # print(x.size())\n",
    "        # input()\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "\n",
    "        # print(\"新tensor\")\n",
    "        out = self.embedding(x[0])\n",
    "   \n",
    "        out = out.unsqueeze(1)\n",
    "\n",
    "        out = torch.cat([self.conv_and_pool(out, conv) for conv in self.convs], 1)\n",
    "        # print(\"zxhfouis\")\n",
    "        # print(out.size())\n",
    "        # input()\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "def reprocfun(content):\n",
    "    reObj = re.compile(\"@.*?(?=：| |:)\")\n",
    "    reObj2 = re.compile(\"<.*?(?=>)\")\n",
    "    x=reObj.sub(\"\",content)\n",
    "    x = reObj2.sub(\"\",x)\n",
    "    pattern = re.compile(r'[^\\u4e00-\\u9fa5]')\n",
    "    text = re.sub(pattern,'',x)\n",
    "    # text = text.replace(\"啊\",\"\")\n",
    "    # text = text.replace(\"呢\",\"\")\n",
    "    # text = text.replace(\"啦\",\"\")\n",
    "    # text = text.replace(\"的\",\"\")\n",
    "    # text = text.replace(\"是\",\"\")\n",
    "    # text = text.replace(\"呀\",\"\")\n",
    "    # text = text.replace(\"了\",\"\")\n",
    "    # text = text.replace(\"和\",\"\")\n",
    "    # text = text.replace(\"又\",\"\")\n",
    "    return text\n",
    "\n",
    "def build_vocab(file_path, tokenizer, max_size, min_freq):#    tokenizer = lambda x: [y for y in x]  \n",
    "    vocab_dic = {}\n",
    "    this_dataset = {}\n",
    "    dataset_path = \"E:/workspace/datamining/test/MYETEST/grA/datasets/weibo_senti_100k\"\n",
    "    files= os.listdir(dataset_path)\n",
    "    for file in files:\n",
    "        path = os.path.join(dataset_path, file)\n",
    "        if not os.path.isdir(path) and not file[0] == '.': # 跳过隐藏文件和文件夹\n",
    "            f = open(path, 'r',  encoding='UTF-8') # 打开文件\n",
    "            for line in f.readlines():\n",
    "                 this_dataset[line] = file[:-4]\n",
    "    #types = (\"科技\", \"社会\", \"娱乐\", \"财经\", \"体育\")\n",
    "    # types = (\"正向\",\"负向\")\n",
    "    types = (\"愤怒\",\"害怕\",\"惊讶\",\"开心\",\"难过\",\"喜欢\",\"厌恶\")\n",
    "    for k, v in tqdm(list(this_dataset.items())):\n",
    "        # print(k, \"Type:\", v, '\\n')\n",
    "            # break\n",
    "        lin = k.strip()\n",
    "        if not lin:\n",
    "            continue\n",
    "        content = lin\n",
    "        # pattern = re.compile(r'[^\\u4e00-\\u9fa5]')\n",
    "        # # print(content.replace(\"~!@#$%^&*()_+`}{|\\[\\]\\:\\\";\\-\\\\\\='<>?,./，,!。、][《》？；：‘“{【】}|、！@#￥%……&*（）——+=-\",\"\"))\n",
    "        # print(content)\n",
    "        # tem = re.sub(pattern,'',content)\n",
    "        # print(tem)\n",
    "\n",
    "        # input()\n",
    "        content = reprocfun(content)\n",
    "\n",
    "        for word in tokenizer(content):\n",
    "            vocab_dic[word] = vocab_dic.get(word, 0) + 1\n",
    "    vocab_list = sorted([_ for _ in vocab_dic.items() if _[1] >= min_freq], key=lambda x: x[1], reverse=True)[:max_size]\n",
    "    vocab_dic = {word_count[0]: idx for idx, word_count in enumerate(vocab_list)}#enum编号\n",
    "    vocab_dic.update({UNK: len(vocab_dic), PAD: len(vocab_dic) + 1})    \n",
    "    return vocab_dic\n",
    "def build_dataset(config):\n",
    "    tokenizer = lambda x: [y for y in x]\n",
    "    if os.path.exists(config.vocab_path):\n",
    "        vocab = pkl.load(open(config.vocab_path, 'rb'))\n",
    "    else:\n",
    "        vocab = build_vocab(config.train_path, tokenizer=tokenizer, max_size=MAX_VOCAB_SIZE, min_freq=1)\n",
    "        with open( 'mydata/data/vocab3.pkl', 'wb') as f:\n",
    "            pkl.dump(vocab, f, pkl.HIGHEST_PROTOCOL)\n",
    "        # pkl.dump(vocab, open(config.vocab_path, 'wb'))\n",
    "    print(f\"Vocab size: {len(vocab)}\")\n",
    "\n",
    "\n",
    "    pad_size = 32 \n",
    "    def ld_fun(data_list):\n",
    "        contents = []\n",
    "        for line in tqdm(data_list):\n",
    "            #lin = line.strip()\n",
    "\n",
    "            try:\n",
    "                content,label = line.split('\\t')#content 为前面的新闻，label为后面标签\n",
    "                # content = content.replace(\",，。. \",\"\")\n",
    "                content = reprocfun(content)\n",
    "            except:\n",
    "                print(line)\n",
    "                print(\"dfshiou\")\n",
    "                input()\n",
    "            words_line = []\n",
    "            token = tokenizer(content)#tokenizer = lambda x: [y for y in x]，拆成单字\n",
    "            seq_len = len(token)\n",
    "            if pad_size:#短补长截,短的字符长度不变，长的截断为pad_size\n",
    "                if len(token) < pad_size:\n",
    "                    token.extend([PAD] * (pad_size - len(token)))\n",
    "                else:\n",
    "                    token = token[:pad_size]\n",
    "                    seq_len = pad_size\n",
    "            # word to id\n",
    "            for word in token:\n",
    "                words_line.append(vocab.get(word, vocab.get(UNK)))\n",
    "            contents.append((words_line, int(label), seq_len))\n",
    "        return contents\n",
    "    sentences = [] # 新闻文本\n",
    "    target = [] # 类别\n",
    "    dataset_path = 'datasets/weibo_senti_100k'\n",
    "    # labels = {'负向': '0', '正向': '1'}\n",
    "   # labels = {'负向': '0', '正向': '1'}\n",
    "    labels = { '喜欢': '0','厌恶':'1','开心':'2','难过':'3','愤怒':'4','惊讶':'5','害怕':'6'}\n",
    "    files = os.listdir(dataset_path)\n",
    "    for file in files:\n",
    "        path = os.path.join(dataset_path, file)\n",
    "        if not os.path.isdir(path) and not file[0] == '.':\n",
    "            with open(path, 'r', encoding='UTF-8') as f: # 打开文件\n",
    "                for line in f.readlines():\n",
    "                    content = line\n",
    "                    content = content.replace('\\t','')\n",
    "                    content = reprocfun(content)\n",
    "                    content = content+'\\t'\n",
    "                    tags = labels[file[:-4]]\n",
    "                    content = content+tags\n",
    "                    # print(content)\n",
    "                    sentences.append(content)\n",
    "                    \n",
    "\n",
    "    random.shuffle(sentences)\n",
    "    offset = (int)(len(sentences)*0.8)\n",
    "    print(offset)\n",
    "    train_pre = sentences[:offset]\n",
    "    val_pre = sentences[offset:]\n",
    "    train = ld_fun(train_pre)\n",
    "    val = ld_fun(val_pre)\n",
    "    return vocab,train,val       \n",
    "\n",
    "class DatasetIterater(object):#迭代器\n",
    "    def __init__(self, batches, batch_size, device):\n",
    "        self.batch_size = batch_size\n",
    "        self.batches = batches\n",
    "        self.n_batches = len(batches) // batch_size\n",
    "        self.residue = False  # 记录batch数量是否为整数\n",
    "        \n",
    "        self.residue = True\n",
    "        self.index = 0\n",
    "        self.device = device\n",
    "\n",
    "    def _to_tensor(self, datas):\n",
    "        x = torch.LongTensor([_[0] for _ in datas]).to(self.device)\n",
    "        y = torch.LongTensor([_[1] for _ in datas]).to(self.device)\n",
    "\n",
    "        # pad前的长度(超过pad_size的设为pad_size)\n",
    "        seq_len = torch.LongTensor([_[2] for _ in datas]).to(self.device)\n",
    "        return (x, seq_len), y\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.residue and self.index == self.n_batches:\n",
    "            batches = self.batches[self.index * self.batch_size: len(self.batches)]\n",
    "            self.index += 1\n",
    "            batches = self._to_tensor(batches)\n",
    "            return batches\n",
    "\n",
    "        elif self.index >= self.n_batches:\n",
    "            self.index = 0\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            batches = self.batches[self.index * self.batch_size: (self.index + 1) * self.batch_size]\n",
    "            self.index += 1\n",
    "            batches = self._to_tensor(batches)\n",
    "            return batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.residue:\n",
    "            return self.n_batches + 1\n",
    "        else:\n",
    "            return self.n_batches\n",
    "\n",
    "def get_time_dif(start_time):#已经使用时间\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))\n",
    "\n",
    "\n",
    "def build_iterator(dataset, config):\n",
    "    iter = DatasetIterater(dataset, config.batch_size, config.device)\n",
    "    return iter      \n",
    "\n",
    "\n",
    "def train(config, model, train_iter, dev_iter):\n",
    "\n",
    "\n",
    "    # test(config, model, test_iter)\n",
    "    # return 0\n",
    "\n",
    "    # model.load_state_dict(torch.load(config.save_path))\n",
    "    # model.eval()\n",
    "    # for a,b in test_iter:\n",
    "    #     outputs = model(a)\n",
    "    #     x = torch.max(outputs.data, 1)[1].cpu().numpy()\n",
    "    #     print(x)\n",
    "    ##batch_size置1，test路径修改\n",
    "    #return 0\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    # 学习率指数衰减，每次epoch：学习率 = gamma * 学习率\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    total_batch = 0  # 记录进行到多少batch\n",
    "    dev_best_loss = float('inf')\n",
    "    last_improve = 0  # 记录上次验证集loss下降的batch数\n",
    "    flag = False  # 记录是否很久没有效果提升\n",
    "    #writer = SummaryWriter(log_dir=config.log_path + '/' + time.strftime('%m-%d_%H.%M', time.localtime()))\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print('Epoch [{}/{}]'.format(epoch + 1, config.num_epochs))\n",
    "\n",
    "\n",
    "        # scheduler.step() # 学习率衰减\n",
    "        for i, (trains, labels) in enumerate(train_iter):\n",
    "            outputs = model(trains)\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if total_batch % 100 == 0:\n",
    "                # 每多少轮输出在训练集和验证集上的效果\n",
    "                true = labels.data.cpu()\n",
    "                predic = torch.max(outputs.data, 1)[1].cpu()\n",
    "                train_acc = metrics.accuracy_score(true, predic)\n",
    "                dev_acc, dev_loss = evaluate(config, model, dev_iter)\n",
    "                if dev_loss < dev_best_loss:\n",
    "                    dev_best_loss = dev_loss\n",
    "                    torch.save(model.state_dict(), config.save_path,_use_new_zipfile_serialization=False)\n",
    "                    improve = '增长'\n",
    "                    last_improve = total_batch\n",
    "                else:\n",
    "                    improve = ''\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%},  Time: {5} {6}'\n",
    "                print(msg.format(total_batch, loss.item(), train_acc, dev_loss, dev_acc, time_dif, improve))\n",
    "                model.train()\n",
    "            \n",
    "            total_batch += 1\n",
    "            if total_batch - last_improve > config.require_improvement:\n",
    "                # 验证集loss超过1000batch没下降，结束训练\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break\n",
    "        if flag:\n",
    "            break\n",
    "def evaluate(config, model, data_iter, test=False):\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in data_iter:\n",
    "            outputs = model(texts)\n",
    "\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss_total += loss\n",
    "            labels = labels.data.cpu().numpy()\n",
    "            predic = torch.max(outputs.data, 1)[1].cpu().numpy()\n",
    "            if test == True:\n",
    "                print(predic)\n",
    "            labels_all = np.append(labels_all, labels)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "\n",
    "    acc = metrics.accuracy_score(labels_all, predict_all)\n",
    "    if test:\n",
    "        report = metrics.classification_report(labels_all, predict_all, target_names=config.class_list, digits=4)\n",
    "        confusion = metrics.confusion_matrix(labels_all, predict_all)\n",
    "        return acc, loss_total / len(data_iter), report, confusion\n",
    "    return acc, loss_total / len(data_iter)\n",
    "\n",
    "def predict(model,text):\n",
    "\n",
    "\n",
    "\n",
    "    def ld_fun(data_list):\n",
    "        tokenizer = lambda x: [y for y in x]\n",
    "        pad_size = 32\n",
    "        vocab_path= 'mydata/data/vocab3.pkl'\n",
    "        vocab = pkl.load(open(vocab_path, 'rb'))\n",
    "        contents = []\n",
    "        \n",
    "        for line in tqdm(data_list):\n",
    "            #lin = line.strip()\n",
    "            try:\n",
    "                content,label = line.split('\\t')#content 为前面的新闻，label为后面标签\n",
    "            except:\n",
    "                print(line)\n",
    "                print(\"dfshiou\")\n",
    "                input()\n",
    "            words_line = []\n",
    "            token = tokenizer(content)#tokenizer = lambda x: [y for y in x]，拆成单字\n",
    "            seq_len = len(token)\n",
    "            if pad_size:#短补长截,短的字符长度不变，长的截断为pad_size\n",
    "                if len(token) < pad_size:\n",
    "                    token.extend([PAD] * (pad_size - len(token)))\n",
    "                else:\n",
    "                    token = token[:pad_size]\n",
    "                    seq_len = pad_size\n",
    "            # word to id\n",
    "            for word in token:\n",
    "                words_line.append(vocab.get(word, vocab.get(UNK)))\n",
    "            contents.append((words_line, int(label), seq_len))\n",
    "        return contents    \n",
    "    text = text.replace('\\t','')\n",
    "    \n",
    "    content = text\n",
    "    content = content+'\\t'\n",
    "    content = content+'0'\n",
    "    sentences = []\n",
    "    sentences.append(content)\n",
    "\n",
    "    a = 'mydata'\n",
    "    b = '123'\n",
    "    config2 = Config(a,b)\n",
    "    config2.n_vocab = cfggb\n",
    "\n",
    "    mpd = ld_fun(sentences)\n",
    "    mpd_iter = build_iterator(mpd, config2)\n",
    "    \n",
    "    # input()\n",
    "    #labels = {0: '负向', 1: '正向'}\n",
    "    # labels = { 0:'like',1:'disgust',2:'happiness',3:'sadness',4:'anger',5:'surprise',6:'fear'}\n",
    "    labels = { 0:'正向',1:'负向',2:'正向',3:'负向',4:'负向',5:'负向',6:'负向'}\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for a,b in mpd_iter:\n",
    "            outputs = model(a)\n",
    "            x = torch.max(outputs.data, 1)[1].cpu().numpy()\n",
    "            # y = torch.max(outputs.data, 1)[0].cpu().numpy()\n",
    "            outputs.data[0][x[0]] = -999\n",
    "            y = torch.max(outputs.data, 1)[1].cpu().numpy()\n",
    "            print(labels[x[0]],labels[y[0]])\n",
    "            \n",
    "        if str(labels[x[0]])==str(labels[y[0]]):\n",
    "            return str(labels[x[0]])\n",
    "      \n",
    "    return str(labels[x[0]])+\" \"+str(labels[y[0]])+\"(备选)\"\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    # '''\n",
    "    # 下面代码为定式,不需要动\n",
    "    # '''\n",
    "    # a = 'mydata'\n",
    "    # b = '123'\n",
    "    # start_time = time.time()\n",
    "    # torch.manual_seed(1)\n",
    "    # torch.cuda.manual_seed_all(1)\n",
    "    # torch.backends.cudnn.deterministic = True \n",
    "    # config = Config(a,b)\n",
    "    # vocab_path= 'mydata/data/vocab3.pkl'\n",
    "    # vocab = pkl.load(open(vocab_path, 'rb'))\n",
    "    # time_dif = get_time_dif(start_time)\n",
    "    # print(\"Time usage:\", time_dif)\n",
    "    # config.n_vocab = len(vocab)\n",
    "    # global cfggb\n",
    "    # cfggb = len(vocab)\n",
    "    # model = Model(config).to('cpu')\n",
    "    # model.load_state_dict(torch.load(config.save_path))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # text = \"哈哈哈哈哈可是我不太理解哈哈哈哈\"           #在这里输入评论\n",
    "    # predict(model,text) #推导函数,输入(model,text),输出为置信度最高的两种情感\n",
    "    # GUI\n",
    "    def fun(model):#按钮点击的函数    \n",
    "        text = comment_input.get()\n",
    "        text2 = reprocfun(text)\n",
    "        x = predict(model,text2)\n",
    "\n",
    "        print(x)\n",
    "        # predict(model,x)\n",
    "        text3.delete('1.0','1.20')\n",
    "        text3.insert(END,x)        \n",
    "        return 0\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    a = 'mydata'\n",
    "    b = '123'\n",
    "    start_time = time.time()\n",
    "    torch.manual_seed(1)\n",
    "    torch.cuda.manual_seed_all(1)\n",
    "    torch.backends.cudnn.deterministic = True \n",
    "    config = Config(a,b)\n",
    "    # load_datas()\n",
    "    \n",
    "    # vocab,train_data,val_data= build_dataset(config)\n",
    "    # train_iter = build_iterator(train_data, config)\n",
    "    # dev_iter = build_iterator(val_data, config)\n",
    "   \n",
    "    vocab_path= 'mydata/data/vocab3.pkl'\n",
    "    vocab = pkl.load(open(vocab_path, 'rb'))\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "    config.n_vocab = len(vocab)\n",
    "    global cfggb\n",
    "    cfggb = len(vocab)\n",
    "\n",
    "    root = Tk()\n",
    "    \n",
    "    root.title('demo')\n",
    "    root.geometry('600x200+398+279')\n",
    "    Label(root,text='评论内容',font=(15),fg='black').grid()\n",
    "    \n",
    "    Label2 = Label(root,text='情感倾向:',font=(15),fg='black')\n",
    "    Label2.place(rely=0.5, relheight=0.1)\n",
    "    # Label2.grid(row=3,column=0)\n",
    "    comment_input=Entry(root,font=(\"微软雅黑\",10),width=50)\n",
    "    comment_input.place(rely=0.2,relheight=0.2)\n",
    "   # comment_input.pack(\n",
    "    #print(111)\n",
    "    gx = comment_input.get()\n",
    "    \n",
    "\n",
    "    text3 = Text(root)\n",
    "    text3.place(rely=0.6, relheight=0.4)\n",
    "    model = Model(config).to('cpu')\n",
    "    print(model.parameters)\n",
    "   # train(config, model, train_iter, dev_iter)\n",
    "    \n",
    "    model.load_state_dict(torch.load(config.save_path))#加载模型,定式\n",
    "    # predict(model,text)\n",
    "    #predict函数,模型推导,参数为model,text,输出结果为置信度最高的两个类别\n",
    "\n",
    "\n",
    "    #torch.save(model, 'mydata' + '/saved_dict/' + 'cnn2' +''+ '.pt',_use_new_zipfile_serialization=False)\n",
    " \n",
    "    # text = reprocfun(text)\n",
    "    # print(text)\n",
    "    # predict(model,text)\n",
    "    B = Button(root,text =\"OK\", command = lambda:fun(model))\n",
    "    B.place(relx=0.7, rely=0.2, relwidth=0.3, relheight=0.2)\n",
    "    '''\n",
    "        所以正在隔离中的密接的密接怎么说[微笑]\n",
    "        不再随意封校？是真的吗？出校门必须要请假，不正当理由请假出不了校门[允悲]这算是变相封校吗\n",
    "        希望各地能不要在层层加码，一刀切\n",
    "\n",
    "\n",
    "        优化得好，希望执行过程更好。\n",
    "        没想到在长沙居然吃到了大娘水饺，终于有不辣的吃了！\n",
    "\n",
    "        希望疫情快点结束\n",
    "        算是好消息\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
